{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "ea78c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "eeb39b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"TIPS.xlsx\"\n",
    "sixteendata = pd.read_excel(file, sheet_name=\"2016\")\n",
    "seventeendata = pd.read_excel(file, sheet_name=\"2017\")\n",
    "eighteendata = pd.read_excel(file, sheet_name=\"2018\")\n",
    "nineteendata = pd.read_excel(file, sheet_name=\"2019\")\n",
    "twentydata = pd.read_excel(file, sheet_name=\"2020\")\n",
    "twentyonedata = pd.read_excel(file, sheet_name=\"2021\")\n",
    "twentytwodata = pd.read_excel(file, sheet_name=\"2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f3c13208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([sixteendata[\"TIP #1\"],sixteendata[\"TIP #2\"],sixteendata[\"TIP #3\"],sixteendata[\"Anything else you want to mention?\"],seventeendata[\"TIP #1\"],seventeendata[\"TIP #2\"],seventeendata[\"TIP #3\"],seventeendata[\"Anything else you want to mention?\"],eighteendata[\"TIP #1\"],eighteendata[\"TIP #2\"],eighteendata[\"TIP #3\"],eighteendata[\"Anything else you want to mention?\"],nineteendata[\"TIP #1\"],nineteendata[\"TIP #2\"],nineteendata[\"TIP #3\"],nineteendata[\"Anything else you want to mention?\"],twentydata[\"TIP #1\"],twentydata[\"TIP #2\"],twentydata[\"TIP #3\"],twentydata[\"Anything else you want to mention? [this can be more TIPS if you are overflowing with advice]\"],twentyonedata[\"TIP #1\"],twentyonedata[\"TIP #2\"],twentyonedata[\"TIP #3\"],twentyonedata[\"Anything else you want to mention? [this can be more TIPS if you are overflowing with advice]\"],twentytwodata[\"TIP #1\"],twentytwodata[\"TIP #2\"],twentytwodata[\"TIP #3\"],twentytwodata[\"Anything else you want to mention? [this can be more TIPS if you are overflowing with advice]\"]],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "11a3c606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       If you are an international student provide OI...\n",
       "1                  Apply to jobs online over winter break\n",
       "2       When looking for jobs, make sure to emphasize ...\n",
       "3       Everyone here has worked hard and deserves to ...\n",
       "4       It's summer. You're about to think you're busy...\n",
       "                              ...                        \n",
       "1628    This program will take as much time from you a...\n",
       "1629    I am sad to see this amazing year come to an e...\n",
       "1632    Work hard, don't forget to have fun, and cheri...\n",
       "1634    For those who love nature, go kayaking with fr...\n",
       "1635    The days are long, but the year is short. Enjo...\n",
       "Length: 1177, dtype: object"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "2a36e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = []\n",
    "for row in data:\n",
    "        tips.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b0d324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "60b50d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Garrett\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b0cdd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Garrett\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "54a10643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " don\n",
      " learn\n",
      " try\n",
      " work\n",
      " python\n",
      " trust\n",
      " program\n",
      " team\n",
      " practicum\n",
      " notes\n",
      " use\n",
      " start\n",
      " learning\n",
      " time\n",
      " process\n",
      "Cluster 1:\n",
      " time\n",
      " program\n",
      " make\n",
      " friends\n",
      " fun\n",
      " outside\n",
      " classmates\n",
      " know\n",
      " sure\n",
      " enjoy\n",
      " people\n",
      " work\n",
      " hang\n",
      " important\n",
      " things\n",
      "Cluster 2:\n",
      " help\n",
      " faculty\n",
      " ask\n",
      " staff\n",
      " questions\n",
      " don\n",
      " afraid\n",
      " willing\n",
      " know\n",
      " classmates\n",
      " iaa\n",
      " resources\n",
      " want\n",
      " team\n",
      " talk\n",
      "Cluster 3:\n",
      " interview\n",
      " job\n",
      " season\n",
      " companies\n",
      " search\n",
      " networking\n",
      " network\n",
      " interviews\n",
      " process\n",
      " company\n",
      " come\n",
      " alumni\n",
      " don\n",
      " session\n",
      " make\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :15]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298415d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
